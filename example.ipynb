{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Translate a video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare a glossary file\n",
    "\n",
    "If you want to use glossary when translating, place all of them into a directory. The default is `data`. \n",
    "Regardless how many files you have, each file needs to be a `csv` file with at least 4 columns. \n",
    "\n",
    "| Term | Translation | Definition | Example | \n",
    "| --- | --- | --- | --- | \n",
    "| ... | ... | ... | ... |\n",
    "\n",
    "\n",
    "You can add more columns to provide some other information, but these 4 are required for this program to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download or prepare an audio file\n",
    "If you have something you wish to translate, replce the path below. \n",
    "If not, you can download a YouTube video to start with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gat.utils import download_audio\n",
    "\n",
    "audio_file = download_audio(\"url-to-youtube-video\", output_dir=\"data\")\n",
    "# audio_file = \"relative-path-to-your-video-or-audio-file\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transcribe the audio\n",
    "\n",
    "There's two ways to transcribe the audio. \n",
    "One is locally run faster-whisper, the other is calling the OpenAI whisper API. \n",
    "Here, I will show you how to use the locally run model. \n",
    "\n",
    "If your audio have some terms that you wish to be transcribed to, you can do so by providing a prompt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gat.transcription import get_whisper_prompt, transcribe_whisper \n",
    "\n",
    "whisper_prompt = \"You might encounter words like: \" + get_whisper_prompt(\"data\")\n",
    "start, end, text = transcribe_whisper(audio_file, model_size=\"large-v3\", whisper_prompt=whisper_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translate the text\n",
    "\n",
    "Here, there's many different translators. All of them are backed by LLMs. \n",
    "I will demonstrate how to use Ollama, but OpenAI and other OpenAI-compatible services are included as well.\n",
    "The only difference is that you need to provide your own API key in the `.env` file.\n",
    "\n",
    "Before you run the next block of code, make sure you have Ollama running with the model you wish to use downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gat.glossary_matcher import GlossaryMatcher\n",
    "from gat.translators import OllamaTranslator \n",
    "\n",
    "gm = GlossaryMatcher()\n",
    "gm.load_from_dir(\"data\")\n",
    "\n",
    "translator = OllamaTranslator(matcher=gm, model=\"qwen2.5\")\n",
    "translated = translator.translate_sentences(text, n_history=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's next\n",
    "\n",
    "You can now do whatever you want with your translated texts. \n",
    "For instance, you can save them in a srt file for your video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gat.utils import save_srt \n",
    "\n",
    "save_srt(\"your_subtitle_file.srt\", start, end, translated)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
